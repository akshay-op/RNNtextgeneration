{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN text generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPjg+OiH2iwfPMBy0DX5T2D"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3seCbzriI5u"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWguI7Fyk-fe",
        "outputId": "a10bdb16-53e9-4b48-bc55-1d59c080e873"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm9N3mYQk_63",
        "outputId": "9e8067fd-148d-493a-e058-f285568f94fb"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSHnGu7Uk-cF",
        "outputId": "e367a234-f252-4d30-9d0b-34c04d9175f7"
      },
      "source": [
        "print(text[:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkaQkd42mV-x"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h99Vd9wZmhW5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsQlV1vEk-Yh",
        "outputId": "8d4ec1f4-6c8f-406e-c98a-f9857c4e6fc4"
      },
      "source": [
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q18odgHtmihD",
        "outputId": "411f044f-61bd-47f7-e28f-4646465e5552"
      },
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJluvGVfk-V7"
      },
      "source": [
        "seq_length = 100  # length of sequence for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9nRhCwnqrCZ"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8mHkfMOq30W"
      },
      "source": [
        "def split_input_target(chunk):  # for the example: hello\n",
        "    input_text = chunk[:-1]  # hell\n",
        "    target_text = chunk[1:]  # ello\n",
        "    return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOIHj26hrBXU",
        "outputId": "fda3a060-0185-4791-c85a-158dbae2dd92"
      },
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGKPYj_rDRK"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWpz6qqfrX2j",
        "outputId": "c5fac740-6d22-44e1-818e-2898a2fd2419"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgosDhikraKa",
        "outputId": "f39239bd-a181-4112-eeed-96aa50516a18"
      },
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmabarmqtA22",
        "outputId": "94cbc517-a72e-42ed-8921-95dc53d747c8"
      },
      "source": [
        "# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-6.48643635e-03 -6.16371259e-03  9.73703281e-05 ... -4.78683272e-03\n",
            "    1.51243934e-03 -4.01099632e-03]\n",
            "  [-6.32165093e-03 -2.50593456e-03 -9.18984879e-05 ... -5.13757532e-03\n",
            "    5.33785485e-03  1.02609396e-04]\n",
            "  [-5.84340980e-03  3.38461250e-05 -3.96536605e-04 ... -4.89291362e-03\n",
            "    8.31080135e-03  3.50308255e-03]\n",
            "  ...\n",
            "  [-3.87006905e-04  2.88494723e-03 -8.94172769e-03 ...  3.45854042e-03\n",
            "    1.16335147e-03 -9.98336123e-04]\n",
            "  [ 9.28209163e-04 -2.25963304e-03 -1.63321048e-02 ...  6.97530434e-03\n",
            "    4.09109471e-03 -5.08822501e-04]\n",
            "  [-5.87529782e-03 -7.55203189e-03 -1.15630748e-02 ...  1.21748331e-03\n",
            "    6.57962775e-03 -5.05099958e-03]]\n",
            "\n",
            " [[ 5.30495681e-03  4.48574370e-04  2.52465252e-03 ... -3.16061708e-03\n",
            "    5.43312682e-03 -1.14389346e-04]\n",
            "  [ 4.58460208e-03 -1.06487377e-03 -1.84836809e-03 ...  1.70023972e-03\n",
            "    2.42939731e-03 -3.22988303e-03]\n",
            "  [-2.07736017e-03 -4.96585330e-04  2.90052919e-03 ...  5.03088720e-03\n",
            "   -1.55416608e-03 -3.00702569e-03]\n",
            "  ...\n",
            "  [ 5.53049659e-03  2.61664670e-03 -1.99748133e-03 ... -9.47679859e-04\n",
            "    6.44454872e-03 -1.39697976e-02]\n",
            "  [ 8.10987689e-03  1.73412310e-03  1.82430679e-03 ... -2.85055814e-03\n",
            "    9.65454988e-03 -1.27342297e-02]\n",
            "  [ 3.90731078e-03  4.49419953e-03 -2.30917498e-03 ... -3.49475676e-03\n",
            "    2.42210459e-03 -1.39426570e-02]]\n",
            "\n",
            " [[-4.45301551e-03  7.94895401e-04 -4.43579582e-03 ... -2.24848813e-03\n",
            "   -2.56050844e-04  1.99673325e-03]\n",
            "  [ 1.65654300e-03  2.23469338e-03 -1.04623968e-02 ... -1.02241163e-03\n",
            "   -4.88524791e-04  1.37428869e-03]\n",
            "  [-5.75094763e-03 -3.03792395e-03 -7.19390577e-03 ... -5.27175562e-03\n",
            "    8.24274961e-04 -3.30776931e-03]\n",
            "  ...\n",
            "  [ 4.47961967e-04 -4.07848973e-03 -8.68258439e-03 ...  4.82220761e-03\n",
            "    5.02999173e-03 -8.40935577e-03]\n",
            "  [-1.70719763e-03 -3.08789196e-03 -2.97905691e-03 ...  3.92241683e-03\n",
            "    3.69658088e-03 -8.71136505e-03]\n",
            "  [-4.21293965e-03 -1.97949749e-03 -2.41230242e-04 ...  7.83162192e-04\n",
            "    7.39909615e-03 -3.39605054e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 5.30495681e-03  4.48574370e-04  2.52465252e-03 ... -3.16061708e-03\n",
            "    5.43312682e-03 -1.14389346e-04]\n",
            "  [ 4.58811596e-03 -4.92323935e-03 -8.35261587e-03 ...  1.21981208e-03\n",
            "    5.67305321e-03  5.96336322e-04]\n",
            "  [ 3.87765863e-03 -5.26545290e-03 -9.63334739e-03 ...  4.67204908e-03\n",
            "    3.61005915e-03 -2.21332000e-03]\n",
            "  ...\n",
            "  [ 1.73631199e-02  1.82493497e-03  3.53347813e-03 ...  5.73677337e-03\n",
            "    6.33539306e-03 -3.45020031e-04]\n",
            "  [ 1.16005763e-02  4.16673371e-04 -4.32492234e-05 ...  3.70143680e-03\n",
            "    4.62411065e-03 -7.11753964e-03]\n",
            "  [ 9.66365263e-03  4.88763908e-04  4.36706981e-03 ...  1.93320028e-03\n",
            "    6.49950048e-03 -5.97885391e-03]]\n",
            "\n",
            " [[-5.77961514e-03  1.39617850e-03  1.74997817e-03 ...  2.55440525e-03\n",
            "   -2.51990557e-03 -3.71249835e-03]\n",
            "  [ 6.26518391e-04  2.77575268e-03  3.23802489e-03 ... -7.40609830e-04\n",
            "    3.70509597e-03 -3.02936370e-03]\n",
            "  [ 1.58803328e-03  1.12114316e-02 -2.29044259e-03 ... -4.37541120e-03\n",
            "    3.92623339e-03 -1.02701001e-02]\n",
            "  ...\n",
            "  [ 1.49264024e-03  6.56125322e-03 -4.27644607e-03 ... -4.31618467e-03\n",
            "    6.46277843e-03 -9.79034044e-03]\n",
            "  [-6.85941894e-04  5.17419120e-03 -6.77102245e-04 ... -3.32440203e-03\n",
            "    4.53288853e-03 -9.82615445e-03]\n",
            "  [-1.02841249e-03  1.57992973e-03 -3.13823158e-03 ...  2.84900423e-04\n",
            "    1.81477796e-03 -1.03199454e-02]]\n",
            "\n",
            " [[ 4.77581518e-03 -3.79150012e-03 -1.08647381e-03 ... -1.56258652e-03\n",
            "    1.94095774e-04  2.04055617e-03]\n",
            "  [ 2.28675432e-03 -3.07874754e-03 -4.57589235e-03 ... -4.02629143e-03\n",
            "    5.60704153e-04 -3.45315132e-03]\n",
            "  [-2.43403134e-04 -7.38905626e-04 -3.03641288e-03 ... -5.36562037e-03\n",
            "    4.79831547e-03  7.29729480e-04]\n",
            "  ...\n",
            "  [ 9.57734231e-03 -2.88694585e-03  5.49296616e-04 ... -2.15606811e-03\n",
            "    1.09165283e-02 -6.98125968e-03]\n",
            "  [ 7.27785658e-03 -3.92481266e-03 -2.32820120e-03 ...  2.65374128e-03\n",
            "    7.56812561e-03 -9.05089732e-03]\n",
            "  [ 5.46176732e-03 -3.57425120e-03  3.05214128e-03 ... -1.65968668e-04\n",
            "    9.55055002e-03 -7.09639117e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_h22wjpuP9I",
        "outputId": "86d287a1-1c54-41a8-9a89-d9fbd06808ec"
      },
      "source": [
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-6.48643635e-03 -6.16371259e-03  9.73703281e-05 ... -4.78683272e-03\n",
            "   1.51243934e-03 -4.01099632e-03]\n",
            " [-6.32165093e-03 -2.50593456e-03 -9.18984879e-05 ... -5.13757532e-03\n",
            "   5.33785485e-03  1.02609396e-04]\n",
            " [-5.84340980e-03  3.38461250e-05 -3.96536605e-04 ... -4.89291362e-03\n",
            "   8.31080135e-03  3.50308255e-03]\n",
            " ...\n",
            " [-3.87006905e-04  2.88494723e-03 -8.94172769e-03 ...  3.45854042e-03\n",
            "   1.16335147e-03 -9.98336123e-04]\n",
            " [ 9.28209163e-04 -2.25963304e-03 -1.63321048e-02 ...  6.97530434e-03\n",
            "   4.09109471e-03 -5.08822501e-04]\n",
            " [-5.87529782e-03 -7.55203189e-03 -1.15630748e-02 ...  1.21748331e-03\n",
            "   6.57962775e-03 -5.05099958e-03]], shape=(100, 65), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqa0x4vmuU3Q",
        "outputId": "4a453919-53cf-4f21-fa90-e944dd66c3bb"
      },
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-6.4864364e-03 -6.1637126e-03  9.7370328e-05  2.3057193e-03\n",
            " -3.9805258e-03 -2.3698667e-05  4.2010853e-03 -3.1025575e-03\n",
            " -4.1924557e-03 -2.7411757e-03 -8.4670754e-03 -3.1111056e-03\n",
            "  1.1848765e-03 -1.8561697e-03  3.0726618e-03  1.7443453e-03\n",
            " -2.5102342e-03  4.9893139e-03 -3.3055686e-03 -5.5901352e-03\n",
            "  5.0351198e-04  1.5547767e-03 -7.2547263e-03 -1.8744769e-03\n",
            " -5.1742513e-04  1.1339749e-03  3.6851419e-03  3.1041005e-03\n",
            " -5.5878572e-03  1.1287626e-03  5.4616202e-03 -1.1993120e-03\n",
            " -2.3480321e-03  3.4788908e-03  1.6353196e-03 -1.4213037e-03\n",
            "  2.0684318e-03  3.2083679e-03  1.1873455e-03  6.1936360e-03\n",
            "  4.9810754e-03  5.3288299e-03  1.7957556e-03  2.6574805e-03\n",
            " -2.1952745e-03 -2.4874657e-03  9.3412595e-03  1.6059671e-04\n",
            "  1.0050255e-03 -4.0832686e-04  1.3024595e-03 -1.4772383e-03\n",
            " -9.6683030e-04  1.6444469e-03  2.2032941e-03 -1.1217443e-02\n",
            " -2.9116124e-03  1.3983968e-03  1.1706385e-03  2.7717315e-03\n",
            " -1.5075956e-03  2.2751794e-03 -4.7868327e-03  1.5124393e-03\n",
            " -4.0109963e-03], shape=(65,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fCJNnGk0uY19",
        "outputId": "e9a12ea7-dbf2-4db7-8637-55250ed487fc"
      },
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"tpy\\nAYIbRruNxSphc:$iZDoNGtKkXP'nLm;luNU-oU$\\n-RCtGzybIP:GSeHs-BwcvaDx,PFTd:MNxUE3MgDZkvGko'tffnl\\nEL-3\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWSZupfJuirT"
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG1pJ2L8u5vv"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3splG3PvK_X"
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvGLXtPcvNOS",
        "outputId": "43ba075a-b945-4e40-8515-7fb8f4c8d5ad"
      },
      "source": [
        "history = model.fit(data, epochs=50, callbacks=[checkpoint_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 2.6129\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.9235\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.6673\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.5253\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.4395\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.3790\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.3324\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.2932\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.2568\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.2225\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 1.1879\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.1530\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.1167\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.0792\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 1.0407\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 1.0019\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.9612\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.9216\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.8811\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.8425\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.8051\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.7715\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.7385\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.7074\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.6805\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.6548\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.6303\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.6095\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.5914\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.5733\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.5571\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.5415\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.5292\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.5189\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.5090\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4981\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4909\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.4840\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.4758\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4694\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4648\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.4571\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4527\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4478\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 13s 72ms/step - loss: 0.4452\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4399\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4370\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4337\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 13s 70ms/step - loss: 0.4279\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 13s 71ms/step - loss: 0.4256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiS8K3QQve4C"
      },
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-OiQJvFzRIq"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPgmo1IKzTNc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wv2mC5ztpjSy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBmkOOiszVtK"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "    \n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7pSmbhSzt4V",
        "outputId": "8e819262-58c1-49ff-d245-7b39e899c319"
      },
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type a starting string: none\n",
            "noness. Therefore, good friar?\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Go hither to the boornot cry 'D What's that to you?\n",
            "'Tis not proud one I did but the less,\n",
            "While I under the thoughts of this dividengears and pretty boyou speak against\n",
            "Our fire must flee supposes the sin that live?\n",
            "\n",
            "ROMEO:\n",
            "Ay, so Gloucester, lady: lest abson your hate,\n",
            "But but one peor good friar, this is he in his kind\n",
            "And favour of unruly children's bosoms sillies,\n",
            "Didouble action, and his son for thee\n",
            "In the dark backward the Fretzhing nature\n",
            "I can see her beholding to your royal person, have the grace\n",
            "As for Bolingbroke to bid his foe.\n",
            "\n",
            "KING RICHARD II:\n",
            "Go, Bushy, and Sasting to the crown and orcourteman:\n",
            "Let them have scope in quiet rest thou off!\n",
            "\n",
            "First Lord:\n",
            "O Tullus,--\n",
            "\n",
            "Secoved CApullet's incruse, and the wood dopose\n",
            "yet I beherefore yo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPPCwkBhzwR9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_bX8AkRvQtH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qACbxir2k-QI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}